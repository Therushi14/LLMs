{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###RMS Norm"
      ],
      "metadata": {
        "id": "uyUwmLQaLEAV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "COZs71ZqK0M8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self,emb_dim,eps=1e-5):\n",
        "    super().__init__() # Call super() as a function\n",
        "    self.eps = eps\n",
        "    self.emb_dim = emb_dim\n",
        "    self.weight = nn.Parameter(torch.ones(emb_dim)).float()\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Calculate mean square\n",
        "    means = x.pow(2).mean(dim=-1,keepdim=True)\n",
        "    x_normed = x * torch.rsqrt(means + self.eps)\n",
        "    return (x_normed * self.weight).to(dtype=x.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "example_batch = torch.randn(2,3,4)\n",
        "\n",
        "rms_norm = RMSNorm(emb_dim=example_batch.shape[-1])\n",
        "rmsnorm_pytorch = torch.nn.RMSNorm(example_batch.shape[-1],eps=1e-5)\n",
        "\n",
        "assert torch.allclose(rms_norm(example_batch),rmsnorm_pytorch(example_batch))"
      ],
      "metadata": {
        "id": "kFJQbghGK4i-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SiLU Activation"
      ],
      "metadata": {
        "id": "i__b_N38K4f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SiLU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SiLU,self).__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x*torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "8CSycbtFK4ck"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silu = SiLU()\n",
        "assert torch.allclose(silu(example_batch),torch.nn.functional.silu(example_batch))"
      ],
      "metadata": {
        "id": "mgPBWbv0K4Zl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FeedForward module"
      ],
      "metadata": {
        "id": "M5g1tThUK4Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.silu = SiLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "      x_fc1 = self.fc1(x)\n",
        "      x_fc2 = self.fc2(x)\n",
        "      x = self.silu(x_fc1) * x_fc2\n",
        "      return self.fc3(x)"
      ],
      "metadata": {
        "id": "ZTdINtcIK4Ts"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RoPE"
      ],
      "metadata": {
        "id": "I8ssaLlxK4Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def precompute_rope_params(head_dim,theta_base=10_000,context_length=4096):\n",
        "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
        "\n",
        "    # Compute the inverse frequencies\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
        "\n",
        "    #Generate position indices\n",
        "    positions = torch.arange(context_length)\n",
        "\n",
        "    # Compute the angles\n",
        "    angles = positions[:,None] * inv_freq[None,:]\n",
        "\n",
        "    # Expand angles to match the head_dim\n",
        "    angles = torch.cat([angles,angles],dim=1)\n",
        "\n",
        "    # Precompute sine and cosine\n",
        "    cos = torch.cos(angles)\n",
        "    sin = torch.sin(angles)\n",
        "\n",
        "    return cos,sin\n",
        "\n",
        "def compute_rope(x, cos, sin):\n",
        "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
        "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
        "\n",
        "    # Split x into first half and second half\n",
        "    x1 = x[..., : head_dim // 2]  # First half\n",
        "    x2 = x[..., head_dim // 2 :]  # Second half\n",
        "\n",
        "    # Adjust sin and cos shapes\n",
        "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
        "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Apply the rotary transformation\n",
        "    rotated = torch.cat((-x2, x1), dim=-1)\n",
        "    x_rotated = (x * cos) + (rotated * sin)\n",
        "\n",
        "    return x_rotated.to(dtype=x.dtype)\n"
      ],
      "metadata": {
        "id": "bvPGpxJgK4K8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "context_len = 5\n",
        "num_heads = 4\n",
        "head_dim = 16\n",
        "\n",
        "cos,sin = precompute_rope_params(head_dim=head_dim,context_length=context_len)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "queries = torch.randn(batch_size,num_heads,context_len,head_dim)\n",
        "keys = torch.randn(batch_size,num_heads,context_len,head_dim)\n",
        "\n",
        "queries_rot = compute_rope(queries,cos,sin)\n",
        "keys_rot = compute_rope(keys,cos,sin)"
      ],
      "metadata": {
        "id": "vOmbG3aGK4H8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MultiHeadAttention"
      ],
      "metadata": {
        "id": "QAoClem0cJ90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, num_heads, dtype=None):  # ,dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)  # Linear layer to combine head outputs\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length)\n",
        "        self.register_buffer(\"cos\", cos)\n",
        "        self.register_buffer(\"sin\", sin)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        keys = compute_rope(keys, self.cos, self.sin)\n",
        "        queries = compute_rope(queries, self.cos, self.sin)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        # attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "zC0lp-VqK4FN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "context_len = 100\n",
        "max_context_len = 4096\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "\n",
        "example_batch = torch.randn((batch_size,context_len,embed_dim))\n",
        "\n",
        "mha = MultiHeadAttention(\n",
        "    d_in = embed_dim,\n",
        "    d_out = embed_dim,\n",
        "    context_length=max_context_len,\n",
        "    num_heads = num_heads\n",
        ")\n",
        "\n",
        "mha(example_batch)\n",
        "\n",
        "del mha"
      ],
      "metadata": {
        "id": "fLar1pMbbsoj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Block"
      ],
      "metadata": {
        "id": "LDqYyCs4cGjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dtype=cfg[\"dtype\"]\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "\n",
        "\n",
        "        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "KPfJPVk3cTTr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Class"
      ],
      "metadata": {
        "id": "F4mfWu86cTRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Llama2Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
        "\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        x = tok_embeds\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "zHCECDHqcTOm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLAMA2_CONFIG_7B = {\n",
        "    \"vocab_size\": 32000,     # Vocabulary size\n",
        "    \"context_length\": 4096,  # Context length\n",
        "    \"emb_dim\": 4096,         # Embedding dimension\n",
        "    \"n_heads\": 32,           # Number of attention heads\n",
        "    \"n_layers\": 32,          # Number of layers\n",
        "    \"hidden_dim\": 11008,     # NEW: Size of the intermediate dimension in FeedForward\n",
        "    \"dtype\": torch.bfloat16  # NEW: Lower-precision dtype to reduce memory usage\n",
        "}"
      ],
      "metadata": {
        "id": "c6tlhZVhcTLt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"GPU is NOT available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXib6UnhdtIL",
        "outputId": "d333c810-9ed2-4e72-c13f-21f4b115471c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Llama2Model(LLAMA2_CONFIG_7B)"
      ],
      "metadata": {
        "id": "okMsSwyTcTI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "id": "sIfETmdNcTF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_memory_size(model, input_dtype=torch.float32):\n",
        "    total_params = 0\n",
        "    total_grads = 0\n",
        "    for param in model.parameters():\n",
        "        # Calculate total number of elements per parameter\n",
        "        param_size = param.numel()\n",
        "        total_params += param_size\n",
        "        # Check if gradients are stored for this parameter\n",
        "        if param.requires_grad:\n",
        "            total_grads += param_size\n",
        "\n",
        "    # Calculate buffer size (non-parameters that require memory)\n",
        "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "\n",
        "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
        "    # We assume parameters and gradients are stored in the same type as input dtype\n",
        "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
        "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "\n",
        "    # Convert bytes to gigabytes\n",
        "    total_memory_gb = total_memory_bytes / (1024**3)\n",
        "\n",
        "    return total_memory_gb\n",
        "\n",
        "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
        "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
      ],
      "metadata": {
        "id": "FeuP_c9OdXPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0jK5rYudcA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "53X31oWhdb-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AVmQVZq3db7N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}